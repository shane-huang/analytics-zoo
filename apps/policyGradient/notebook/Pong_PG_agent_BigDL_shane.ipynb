{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T06:04:49.427844Z",
     "start_time": "2017-11-13T06:04:48.978585Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from bigdl.dataset.transformer import Sample\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.nn.initialization_method import *\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from rl.criterion import *\n",
    "\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T06:04:49.872869Z",
     "start_time": "2017-11-13T06:04:49.866052Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_RDD(X, y):\n",
    "    return sc.parallelize(X).zip(sc.parallelize(y)).map(\n",
    "            lambda x: Sample.from_ndarray(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T06:04:50.348341Z",
     "start_time": "2017-11-13T06:04:50.339044Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    I=I[35:195]\n",
    "    I=I[::2,::2,0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0] = 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T06:04:50.993031Z",
     "start_time": "2017-11-13T06:04:50.958691Z"
    }
   },
   "outputs": [],
   "source": [
    "class PGAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.95, epsilon=1.0,\n",
    "                 epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.01,\n",
    "                load=False, load_path='/tmp/model.bigl'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_model()\n",
    "        if load:\n",
    "            self.model = self.model.load(load_path)\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Linear(6400, 200))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Linear(200, 1))\n",
    "        model.add(Sigmoid())\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        result = self.model.forward(state)\n",
    "        return 2 if result<np.random.random() else 3\n",
    "    \n",
    "    def save(self,name='model'+str(datetime.now())[:-7]+'bigdl'):\n",
    "        self.model.save(\"/tmp/\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T06:04:51.618321Z",
     "start_time": "2017-11-13T06:04:51.589209Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_game(agent, render=False, report_score=False):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    prev_obs = np.zeros(6400)\n",
    "    states = [np.zeros(6400)]\n",
    "    score = [0,0]\n",
    "    while not done:\n",
    "        if render == True:\n",
    "            env.render()\n",
    "        cur_obs = prepro(state)\n",
    "        cur_state = cur_obs - prev_obs\n",
    "        prev_obs = cur_obs\n",
    "        states.append(cur_state)\n",
    "        action = agent.act(cur_state)\n",
    "        actions.append(action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if reward == -1:\n",
    "            score[0] += 1\n",
    "        elif reward == 1:\n",
    "            score[1] += 1\n",
    "        rewards.append(reward)\n",
    "    if report_score:\n",
    "        print('No. of -1 score vs. 1 score is {} : {}'.format(score[0], score[1]))\n",
    "    return states[1:], actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T06:04:53.244581Z",
     "start_time": "2017-11-13T06:04:53.233744Z"
    }
   },
   "outputs": [],
   "source": [
    "def running_reward(actions, rewards, gamma):\n",
    "    result = []\n",
    "    run_rew = 0\n",
    "    for action, reward in list(zip(actions, rewards))[::-1]:\n",
    "        if reward != 0:\n",
    "            run_rew = 0\n",
    "        run_rew = run_rew*gamma + reward\n",
    "        result.append([action, run_rew])\n",
    "    return np.vstack(result[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T06:04:54.609282Z",
     "start_time": "2017-11-13T06:04:54.595086Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_n_games(agent, n=20, report_score=False):\n",
    "    X_batch = np.array(np.zeros(6400))\n",
    "    y_batch = np.array([0,0])\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        a, b, c = play_game(agent, report_score=report_score)\n",
    "        X_batch = np.vstack((X_batch, a))\n",
    "        y_batch = np.vstack((y_batch, running_reward(b, c, agent.gamma)))\n",
    "        results.append(np.sum(c))\n",
    "    return X_batch[1:], y_batch[1:], np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T06:04:55.825419Z",
     "start_time": "2017-11-13T06:04:55.216850Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-13 14:04:55,221] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createLinear\n",
      "creating: createReLU\n",
      "creating: createLinear\n",
      "creating: createSigmoid\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "sc = SparkContext.getOrCreate(create_spark_conf())\n",
    "state_size = 6400\n",
    "action_size = 2\n",
    "agent = PGAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T06:29:09.220076Z",
     "start_time": "2017-11-13T06:04:55.924043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 2\n",
      "No. of -1 score vs. 1 score is 21 : 1\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 2\n",
      "No. of -1 score vs. 1 score is 21 : 3\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 1 result is -20.2\n",
      "using batch_size =  14384\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 1\n",
      "No. of -1 score vs. 1 score is 21 : 1\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 2 result is -20.8\n",
      "using batch_size =  12656\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 3 result is -21.0\n",
      "using batch_size =  10184\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 4 result is -21.0\n",
      "using batch_size =  10200\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 5 result is -21.0\n",
      "using batch_size =  10192\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 6 result is -21.0\n",
      "using batch_size =  10200\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 7 result is -21.0\n",
      "using batch_size =  10264\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 8 result is -21.0\n",
      "using batch_size =  10192\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 9 result is -21.0\n",
      "using batch_size =  10200\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 10 result is -21.0\n",
      "using batch_size =  10128\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 11 result is -21.0\n",
      "using batch_size =  10224\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 12 result is -21.0\n",
      "using batch_size =  10184\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 13 result is -21.0\n",
      "using batch_size =  10184\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 14 result is -21.0\n",
      "using batch_size =  10224\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 15 result is -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using batch_size =  10176\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 16 result is -21.0\n",
      "using batch_size =  10192\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 17 result is -21.0\n",
      "using batch_size =  10144\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 18 result is -21.0\n",
      "using batch_size =  10160\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "No. of -1 score vs. 1 score is 21 : 0\n",
      "Result of update no. 19 result is -21.0\n",
      "using batch_size =  10224\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxIteration\n",
      "creating: createOptimizer\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0f8181bed205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu't = 0\\ncore_num = 8 #set core number as optimization requires batch_size to be multiple of core_num\\nwhile True:\\n    t +=1\\n    X_batch, y_batch, result = play_n_games(agent, n=10, report_score=True)\\n    print \"Result of update no.\",t,\"result is\",result\\n    #print(t, result)\\n    if result > 0:\\n        break\\n    rdd_sample = to_RDD(X_batch, y_batch)\\n    \\n    #calculate batch size as the latest multiple of core_num that\\'s less than no.of samples\\n    #batch_size = 1000\\n    batch_size = X_batch.shape[0] - X_batch.shape[0]%core_num\\n    print \"using batch_size = \",batch_size\\n    \\n    optimizer = Optimizer(model=agent.model,\\n                                  training_rdd=rdd_sample,\\n                                  criterion=VanillaPGCriterion(clipping=False,size_average=True),\\n                                  optim_method=Adam(learningrate=agent.learning_rate),\\n                                  end_trigger=MaxIteration(1),\\n                                  batch_size=batch_size)\\n    agent.model = optimizer.optimize()\\n    if t % 5 == 0:\\n        agent.model.save(\"PG_pong_NN\", True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2101\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7bd19e4f916a>\u001b[0m in \u001b[0;36mplay_n_games\u001b[0;34m(agent, n, report_score)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-20acda63c183>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(agent, render, report_score)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprev_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5ec9c1fa894f>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'bigdl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = 0\n",
    "core_num = 8 #set core number as optimization requires batch_size to be multiple of core_num\n",
    "while True:\n",
    "    t +=1\n",
    "    X_batch, y_batch, result = play_n_games(agent, n=10, report_score=True)\n",
    "    print \"Result of update no.\",t,\"result is\",result\n",
    "    #print(t, result)\n",
    "    if result > 0:\n",
    "        break\n",
    "    rdd_sample = to_RDD(X_batch, y_batch)\n",
    "    \n",
    "    #calculate batch size as the latest multiple of core_num that's less than no.of samples\n",
    "    #batch_size = 1000\n",
    "    batch_size = X_batch.shape[0] - X_batch.shape[0]%core_num\n",
    "    print \"using batch_size = \",batch_size\n",
    "    \n",
    "    optimizer = Optimizer(model=agent.model,\n",
    "                                  training_rdd=rdd_sample,\n",
    "                                  criterion=VanillaPGCriterion(clipping=False,size_average=True),\n",
    "                                  optim_method=Adam(learningrate=agent.learning_rate),\n",
    "                                  end_trigger=MaxIteration(1),\n",
    "                                  batch_size=batch_size)\n",
    "    agent.model = optimizer.optimize()\n",
    "    if t % 5 == 0:\n",
    "        agent.model.save(\"PG_pong_NN.model\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient algrorithm with BigDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bigdl.dataset.transformer import Sample\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.nn.criterion import *\n",
    "\n",
    "from rl.criterion import *\n",
    "\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PGAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.95,learning_rate=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Linear(self.state_size, 24))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Linear(24, 24))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Linear(24, 1))\n",
    "        model.add(Sigmoid())\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        result = self.model.forward(state)\n",
    "        return 1 if result > np.random.random() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import to_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_game(agent, render=False):\n",
    "    state = env.reset()\n",
    "    memory = np.array([0,0,0,0])\n",
    "    actions = np.array([])\n",
    "    rewards = np.array([])\n",
    "    for time in range(500):\n",
    "        if render:\n",
    "            env.render()\n",
    "        memory = np.vstack((memory, state))\n",
    "        action = agent.act(state)\n",
    "        actions = np.append(actions, action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            reward = -10\n",
    "        rewards = np.append(rewards, reward)\n",
    "        if done or time == 498:\n",
    "            break\n",
    "    return memory[1:], actions, rewards, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def running_reward(actions, rewards, gamma):\n",
    "    result = []\n",
    "    run_rew = 0\n",
    "    for action, reward in list(zip(actions, rewards))[::-1]:\n",
    "        run_rew = run_rew*gamma + reward\n",
    "        result.append([action, (-1) * run_rew])\n",
    "    return np.vstack(result[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def play_n_games(agent, n=20):\n",
    "    X_batch = np.array([0,0,0,0])\n",
    "    y_batch = np.array([0,0])\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        a, b, c, d = play_game(agent)\n",
    "        X_batch = np.vstack((X_batch, a))\n",
    "        y_batch = np.vstack((y_batch, running_reward(b, c, agent.gamma)))\n",
    "        results.append(d)\n",
    "    return X_batch[1:], y_batch[1:], np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-09 11:04:58,247] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createLinear\n",
      "creating: createReLU\n",
      "creating: createLinear\n",
      "creating: createReLU\n",
      "creating: createLinear\n",
      "creating: createSigmoid\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "sc = SparkContext.getOrCreate(create_spark_conf())\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = PGAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createVanillaPGCriterion\n"
     ]
    }
   ],
   "source": [
    "criterion = VanillaPGCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 19.0\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "2 33.0\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "3 50.0\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "4 25.0\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "5 83.0\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "6 134.0\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "7 158.0\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "8 137.0\n",
      "creating: createVanillaPGCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "9 498.0\n",
      "CPU times: user 1min 49s, sys: 19 s, total: 2min 8s\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = 0\n",
    "while True:\n",
    "    t +=1\n",
    "    X_batch, y_batch, result = play_n_games(agent)\n",
    "    print(t, result)\n",
    "    if result == 498:\n",
    "        break\n",
    "    rdd_sample = to_rdd.to_RDD(X_batch, y_batch, sc)\n",
    "    optimizer = Optimizer(model=agent.model,\n",
    "                                  training_rdd=rdd_sample,\n",
    "                                  criterion=VanillaPGCriterion(),\n",
    "                                  optim_method=Adam(learningrate=agent.learning_rate),\n",
    "                                  end_trigger=MaxEpoch(1),\n",
    "                                  batch_size=40)\n",
    "    agent.model = optimizer.optimize()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
